{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of Backpropagation with MNIST handwritten digits\n",
    "Using only numpy for matrix math, this three layer feedforward neural network can classify MNIST digits at an accuracy of 85 percent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_activation(x, deriv=False):\n",
    "    if not deriv:\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    else:\n",
    "        return np.exp(x) / ((np.exp(x) + 1) **2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet:\n",
    "    def __init__(self, learning_rate):\n",
    "        self.l1weights = 2 * np.random.random([784, 16]) - 1\n",
    "        self.l1biases = 2 * np.random.random([16]) - 1\n",
    "        self.l2weights = 2 * np.random.random([16, 10]) - 1\n",
    "        self.l2biases = 2 * np.random.random([10]) - 1\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "    def fwd_pass(self, X):\n",
    "        l1_activation = sigmoid_activation(np.dot(X, self.l1weights) + self.l1biases)\n",
    "        l2_activation = sigmoid_activation(np.dot(l1_activation, self.l2weights) + self.l2biases)\n",
    "        return l2_activation\n",
    "    \n",
    "    def gradient_descent_step(self, X, Y):\n",
    "        # Forward propagation\n",
    "        \n",
    "        l1_z = np.dot(X, self.l1weights) + self.l1biases\n",
    "        l1_activation = sigmoid_activation(l1_z)\n",
    "        \n",
    "        l2_z = np.dot(l1_activation, self.l2weights) + self.l2biases\n",
    "        l2_activation = sigmoid_activation(l2_z)\n",
    "        \n",
    "        # This is the derivative of our cost function, mean squared error.\n",
    "        # These are the gradients of the final activation layer,\n",
    "        # the derivative of the cost function with respect to the activations.\n",
    "        l2_activation_grad = l2_activation - Y\n",
    "        \n",
    "        # Use the chain rule to compute delta, and multiply the activation gradients\n",
    "        # by the derivative of pre-activations with respect to the activations.\n",
    "        l2_delta = l2_activation_grad * sigmoid_activation(l2_z, deriv=True)\n",
    "        \n",
    "        # Continues with the chain rule to find the layer 2 weight gradient by\n",
    "        # multiplying the previous layer with delta d(wx+b)/dw=x\n",
    "        l2_weight_grad = np.dot(l1_activation.T, l2_delta) / X.shape[0]\n",
    "        \n",
    "        # The bias gradient is delta since the next derivative via the\n",
    "        # chain rule is d(wx+b)/db = 1\n",
    "        l2_bias_grad = np.mean(l2_delta, 0).reshape(self.l2biases.shape)\n",
    "        \n",
    "        # We repeat the process of backpropagating using the chain rule and use\n",
    "        # delta, the derivative of the cost with respect to pre-activations to\n",
    "        # compute the previous layer activation gradient d(wx+b)/dx=w\n",
    "        l1_activation_grad = np.dot(l2_delta, self.l2weights.T)\n",
    "        \n",
    "        # We continue finding the delta, weight, and bias gradients as before of\n",
    "        # the previous layer\n",
    "        l1_delta = l1_activation_grad * sigmoid_activation(l1_z, deriv=True)\n",
    "        l1_weight_grad = np.dot(X.T, l1_delta) / X.shape[0]\n",
    "        l1_bias_grad = np.mean(l1_delta, 0).reshape(self.l1biases.shape)\n",
    "        \n",
    "        # The weights and biases are updated by subtracting their gradients\n",
    "        # multiplied by the learning rate, performing gradient descent\n",
    "        self.l2weights -= self.learning_rate * l2_weight_grad\n",
    "        self.l2biases -= self.learning_rate * l2_bias_grad\n",
    "        \n",
    "        self.l1weights -= self.learning_rate * l1_weight_grad\n",
    "        self.l1biases -= self.learning_rate * l1_bias_grad\n",
    "        \n",
    "    def accuracy(self, X, Y):\n",
    "        predicted_labels = np.argmax(self.fwd_pass(X), 1)\n",
    "        true_labels = np.argmax(Y, 1)\n",
    "        return np.mean(np.equal(predicted_labels, true_labels).astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Accuracy 0.088\n",
      "Epoch 1000 Accuracy 0.372\n",
      "Epoch 2000 Accuracy 0.493\n",
      "Epoch 3000 Accuracy 0.565\n",
      "Epoch 4000 Accuracy 0.618\n",
      "Epoch 5000 Accuracy 0.652\n",
      "Epoch 6000 Accuracy 0.691\n",
      "Epoch 7000 Accuracy 0.728\n",
      "Epoch 8000 Accuracy 0.76\n",
      "Epoch 9000 Accuracy 0.774\n",
      "Epoch 10000 Accuracy 0.792\n",
      "Epoch 11000 Accuracy 0.802\n",
      "Epoch 12000 Accuracy 0.813\n",
      "Epoch 13000 Accuracy 0.825\n",
      "Epoch 14000 Accuracy 0.828\n",
      "Epoch 15000 Accuracy 0.833\n",
      "Epoch 16000 Accuracy 0.835\n",
      "Epoch 17000 Accuracy 0.838\n",
      "Epoch 18000 Accuracy 0.844\n",
      "Epoch 19000 Accuracy 0.848\n",
      "Epoch 19999 Accuracy 0.851\n"
     ]
    }
   ],
   "source": [
    "epochs = 20000\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "display_step = 1000\n",
    "\n",
    "net = NeuralNet(learning_rate)\n",
    "\n",
    "validation_xs, validation_ys = mnist.train.next_batch(1000)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    xs, ys = mnist.train.next_batch(batch_size)\n",
    "    net.gradient_descent_step(xs, ys)\n",
    "    if epoch % display_step == 0:\n",
    "        print(\"Epoch\", epoch, \"Accuracy\", net.accuracy(validation_xs, validation_ys))\n",
    "\n",
    "print(\"Epoch\", epoch, \"Accuracy\", net.accuracy(validation_xs, validation_ys))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
