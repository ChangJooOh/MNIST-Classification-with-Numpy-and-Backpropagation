{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using softmax cross entropy for a better accuracy\n",
    "Implementing softmax cross entropy with MNIST digit recognition boosts the accuracy significantly: up to 92.5% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_activation(x, deriv=False):\n",
    "    if not deriv:\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    else:\n",
    "        return np.exp(x) / ((np.exp(x) + 1) **2)\n",
    "    \n",
    "def softmax(x):\n",
    "    nonlin_x = np.exp(x)\n",
    "    return nonlin_x / np.sum(nonlin_x, 1).reshape(nonlin_x.shape[0], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet:\n",
    "    def __init__(self, learning_rate):\n",
    "        self.l1weights = 2 * np.random.random([784, 16]) - 1\n",
    "        self.l1biases = 2 * np.random.random([16]) - 1\n",
    "        self.l2weights = 2 * np.random.random([16, 10]) - 1\n",
    "        self.l2biases = 2 * np.random.random([10]) - 1\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "    def fwd_pass(self, X):\n",
    "        l1_activation = sigmoid_activation(np.dot(X, self.l1weights) + self.l1biases)\n",
    "        l2_activation = softmax(np.dot(l1_activation, self.l2weights) + self.l2biases)\n",
    "        return l2_activation\n",
    "    \n",
    "    def gradient_descent_step(self, X, Y):\n",
    "        # Forward propagation\n",
    "        \n",
    "        l1_z = np.dot(X, self.l1weights) + self.l1biases\n",
    "        l1_activation = sigmoid_activation(l1_z)\n",
    "        \n",
    "        l2_z = np.dot(l1_activation, self.l2weights) + self.l2biases\n",
    "        l2_activation = softmax(l2_z)\n",
    "        \n",
    "        # Calculates the derivative of softmax crossentropy with respect to the logits\n",
    "        l2_delta = l2_activation - Y\n",
    "        \n",
    "        # Continues with the chain rule to find the layer 2 weight gradient by\n",
    "        # multiplying the previous layer with delta d(wx+b)/dw=x\n",
    "        l2_weight_grad = np.dot(l1_activation.T, l2_delta) / X.shape[0]\n",
    "        \n",
    "        # The bias gradient is delta since the next derivative via the\n",
    "        # chain rule is d(wx+b)/db = 1\n",
    "        l2_bias_grad = np.mean(l2_delta, 0).reshape(self.l2biases.shape)\n",
    "        \n",
    "        # We repeat the process of backpropagating using the chain rule and use\n",
    "        # delta, the derivative of the cost with respect to pre-activations to\n",
    "        # compute the previous layer activation gradient d(wx+b)/dx=w\n",
    "        l1_activation_grad = np.dot(l2_delta, self.l2weights.T)\n",
    "        \n",
    "        # We continue finding the delta, weight, and bias gradients as before of\n",
    "        # the previous layer\n",
    "        l1_delta = l1_activation_grad * sigmoid_activation(l1_z, deriv=True)\n",
    "        l1_weight_grad = np.dot(X.T, l1_delta) / X.shape[0]\n",
    "        l1_bias_grad = np.mean(l1_delta, 0).reshape(self.l1biases.shape)\n",
    "        \n",
    "        # The weights and biases are updated by subtracting their gradients\n",
    "        # multiplied by the learning rate, performing gradient descent\n",
    "        self.l2weights -= self.learning_rate * l2_weight_grad\n",
    "        self.l2biases -= self.learning_rate * l2_bias_grad\n",
    "        \n",
    "        self.l1weights -= self.learning_rate * l1_weight_grad\n",
    "        self.l1biases -= self.learning_rate * l1_bias_grad\n",
    "        \n",
    "    def accuracy(self, X, Y):\n",
    "        predicted_labels = np.argmax(self.fwd_pass(X), 1)\n",
    "        true_labels = np.argmax(Y, 1)\n",
    "        return np.mean(np.equal(predicted_labels, true_labels).astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Accuracy 0.116\n",
      "Epoch 1000 Accuracy 0.785\n",
      "Epoch 2000 Accuracy 0.832\n",
      "Epoch 3000 Accuracy 0.86\n",
      "Epoch 4000 Accuracy 0.872\n",
      "Epoch 5000 Accuracy 0.876\n",
      "Epoch 6000 Accuracy 0.885\n",
      "Epoch 7000 Accuracy 0.889\n",
      "Epoch 8000 Accuracy 0.899\n",
      "Epoch 9000 Accuracy 0.912\n",
      "Epoch 10000 Accuracy 0.912\n",
      "Epoch 11000 Accuracy 0.914\n",
      "Epoch 12000 Accuracy 0.914\n",
      "Epoch 13000 Accuracy 0.916\n",
      "Epoch 14000 Accuracy 0.921\n",
      "Epoch 15000 Accuracy 0.921\n",
      "Epoch 16000 Accuracy 0.922\n",
      "Epoch 17000 Accuracy 0.926\n",
      "Epoch 18000 Accuracy 0.921\n",
      "Epoch 19000 Accuracy 0.922\n",
      "Epoch 19999 Accuracy 0.925\n"
     ]
    }
   ],
   "source": [
    "epochs = 20000\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "display_step = 1000\n",
    "\n",
    "net = NeuralNet(learning_rate)\n",
    "\n",
    "validation_xs, validation_ys = mnist.train.next_batch(1000)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    xs, ys = mnist.train.next_batch(batch_size)\n",
    "    net.gradient_descent_step(xs, ys)\n",
    "    if epoch % display_step == 0:\n",
    "        print(\"Epoch\", epoch, \"Accuracy\", net.accuracy(validation_xs, validation_ys))\n",
    "\n",
    "print(\"Epoch\", epoch, \"Accuracy\", net.accuracy(validation_xs, validation_ys))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
